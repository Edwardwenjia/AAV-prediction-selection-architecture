{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e4bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "from pylab import xticks,yticks,np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import gaussian_kde\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from utils_f4f import si_format\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea156d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "assay = 'nor_package'\n",
    "array = 'AA_sequence'\n",
    "\n",
    "df_all = pd.read_csv('../data/1st_780w_packseq_aa.tsv',delimiter='\\t')\n",
    "df_all = df_all.rename(columns={'aa':'AA_sequence'})\n",
    "df = df_all[['AA_sequence', 'nor_package']]\n",
    "df= df.sort_values(by='nor_package')\n",
    "\n",
    "#Fill in the nan data and replace the inf value\n",
    "df['nor_package'] = df['nor_package'].fillna(0)  \n",
    "df['nor_package'] = df['nor_package'].replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=['nor_package','AA_sequence'])\n",
    "\n",
    "#Stratified sampling\n",
    "df = df.iloc[::5]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_package = df_all[np.isfinite(df_all[assay])][assay]\n",
    "data = np.array(nor_package) \n",
    "data = data.reshape(-1, 1)  # Convert the data into a column vector\n",
    "\n",
    "# Fit the data using GaussianMixture\n",
    "n_components = 2 # Set the number of components for the hybrid model\n",
    "gmm = GaussianMixture(n_components=n_components)\n",
    "gmm.fit(data)\n",
    "\n",
    "# Get the mean and covariance matrix for each component\n",
    "means = gmm.means_\n",
    "covariances = gmm.covariances_\n",
    "\n",
    "# Generate the fitted distribution\n",
    "x = np.linspace(-10, 5, 10000)\n",
    "x = x.reshape(-1, 1)\n",
    "y = np.exp(gmm.score_samples(x))\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(2,1.7),dpi=100)\n",
    "# Plot the raw data and the distribution after fitting\n",
    "plt.hist(data, bins=100, density=True, alpha=0.6, color = '#BAB3A3',label='Prediction')\n",
    "\n",
    "# Plot the normal distribution for each component\n",
    "for i in range(n_components):\n",
    "    component = np.exp(-(x - means[i]) ** 2 / (2 * covariances[i]))\n",
    "    component /= np.sqrt(2 * np.pi * covariances[i])\n",
    "    component *= gmm.weights_[i]\n",
    "    if i == 0:\n",
    "        ax.plot(x, component, '-', label=f'Distribution {i + 1}', color='#F66E68', alpha=1)\n",
    "        ax.fill_between(x.flatten(), 0, component.flatten(), color='#F66E68', alpha=0.4)\n",
    "    else:\n",
    "        ax.plot(x, component, '-', label=f'Distribution {i + 1}', color='#457B9D', alpha=1)\n",
    "        ax.fill_between(x.flatten(), 0, component.flatten(), color='#457B9D', alpha=0.4)\n",
    "\n",
    "\n",
    "plt.title('Truth_distribution',fontsize=10)\n",
    "ax.tick_params(axis='both', which='both', length=2,labelsize=8)\n",
    "ax.set_xticks([-5, 0, 5]); \n",
    "# ax.set_yticks([])\n",
    "plt.ylabel('Density',labelpad=2,fontsize=10)\n",
    "plt.xlabel('Nor_package',labelpad=2,fontsize=10)\n",
    "plt.legend(fontsize=7,frameon=False,loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03334734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def onehot_flatten(integer_vec, alphabet):\n",
    "    num_labels = len(alphabet)\n",
    "    one_hot = np.zeros((len(integer_vec), num_labels))\n",
    "    one_hot[np.arange(len(integer_vec)), integer_vec] = 1\n",
    "    return one_hot.flatten()\n",
    "\n",
    "def seq_to_onehot(char_sequences, alphabet):\n",
    "    char_to_int = {c: i for i, c in enumerate(alphabet)}\n",
    "    integer_encoded = [[char_to_int[char] for char in seq] for seq in char_sequences]\n",
    "    integer_encoded = pd.DataFrame(integer_encoded)\n",
    "    \n",
    "    one_hot_encoded_list = []\n",
    "    for index, row in integer_encoded.iterrows():\n",
    "        one_hot_encoded_list.append(onehot_flatten(row, alphabet))\n",
    "    \n",
    "    one_hot_encoded = pd.DataFrame(one_hot_encoded_list, dtype='int')\n",
    "    \n",
    "    return one_hot_encoded, integer_encoded\n",
    "\n",
    "# Define alphabet\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "# Apply one-hot encoding\n",
    "X, integer_encoded = seq_to_onehot(list(df['AA_sequence']), alphabet=alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812a8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_values_list  = X\n",
    "nor_package_values_list = df['nor_package']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example training data\n",
    "X_train = np.array(aa_values_list)\n",
    "y_train = np.array(nor_package_values_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),  # Batch Normalization layer\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),  # Batch Normalization layer\n",
    "    tf.keras.layers.Dense(1, activation='relu')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_activity = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predicted_activity)\n",
    "r2 = r2_score(y_test, predicted_activity)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d3e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84276cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Reshape X_train to have a time dimension\n",
    "\n",
    "X_train = np.array(aa_values_list)\n",
    "y_train = np.array(nor_package_values_list)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Build the improved RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_activity = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predicted_activity)\n",
    "r2 = r2_score(y_test, predicted_activity)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Reshape X_train to have a time dimension\n",
    "\n",
    "X_train = np.array(aa_values_list)\n",
    "y_train = np.array(nor_package_values_list)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Build the improved RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(128, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
    "    tf.keras.layers.GRU(64, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted_activity = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predicted_activity)\n",
    "r2 = r2_score(y_test, predicted_activity)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af85166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1 = pd.DataFrame(predicted_activity)\n",
    "df2 = pd.DataFrame(y_test)\n",
    "\n",
    "random_indices = np.random.choice(len(df1), size=10000, replace=False)\n",
    "df1_sampled = df1.iloc[random_indices]\n",
    "df2_sampled = df2.iloc[random_indices]\n",
    "\n",
    "# 对数据进行归一化\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(df1_sampled.values.reshape(-1, 1)).flatten()\n",
    "y_scaled = scaler.fit_transform(df2_sampled.values.reshape(-1, 1)).flatten()\n",
    "xy = np.vstack([x_scaled, y_scaled])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "idx = z.argsort()\n",
    "x_scaled, y_scaled, z = x_scaled[idx], y_scaled[idx], z[idx]\n",
    "plt.style.use('nature')\n",
    "fig, ax = plt.subplots(figsize=(3, 3), dpi=300)\n",
    "scatter = ax.scatter(x_scaled, y_scaled, marker='o', c=z, edgecolors='none', s=10, cmap='Spectral_r', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Turth (Normalized)')\n",
    "ax.set_ylabel('Predictions (Normalized)')\n",
    "ax.set_title('Correlation +  GRU')\n",
    "\n",
    "correlation = df1.corrwith(df2)\n",
    "print(\"结果相关性：\", correlation)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = fig.colorbar(scatter, cax=cax, label='Frequency')\n",
    "ax.text(0.3, 0.9, f'Correlation: {correlation.values[0]:.4f}', transform=ax.transAxes, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.8))\n",
    "ax.plot([-2, 2], [-2, 2], ls=\"--\", )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.style.use('nature')\n",
    "data = np.array(df1) \n",
    "data = data.reshape(-1, 1)  # 将数据转换成列向量的形式\n",
    "\n",
    "# 使用GaussianMixture拟合数据\n",
    "n_components = 2 # 设置混合模型的分量个数\n",
    "gmm = GaussianMixture(n_components=n_components)\n",
    "gmm.fit(data)\n",
    "\n",
    "# 获取每个分量的均值和协方差矩阵\n",
    "means = gmm.means_\n",
    "covariances = gmm.covariances_\n",
    "\n",
    "# 生成拟合后的分布\n",
    "x = np.linspace(-7, 5, 100000)\n",
    "x = x.reshape(-1, 1)\n",
    "y = np.exp(gmm.score_samples(x))\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(3,2.4),dpi=300)\n",
    "# 绘制原始数据和拟合后的分布\n",
    "plt.hist(data, bins=100, density=True, alpha=0.6, color = '#BAB3A3',label='Prediction')\n",
    "#plt.plot(x, y, '-r', label='GMM fit')\n",
    "\n",
    "\n",
    "# 绘制每个分量的正态分布\n",
    "for i in range(n_components):\n",
    "    component = np.exp(-(x - means[i]) ** 2 / (2 * covariances[i]))\n",
    "    component /= np.sqrt(2 * np.pi * covariances[i])\n",
    "    component *= gmm.weights_[i]\n",
    "    if i == 0:\n",
    "        ax.plot(x, component, '-', label=f'Distribution {i + 1}', color='#F66E68', alpha=1)\n",
    "        ax.fill_between(x.flatten(), 0, component.flatten(), color='#F66E68', alpha=0.4)\n",
    "    else:\n",
    "        ax.plot(x, component, '-', label=f'Distribution {i + 1}', color='#457B9D', alpha=1)\n",
    "        ax.fill_between(x.flatten(), 0, component.flatten(), color='#457B9D', alpha=0.4)\n",
    "\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "plt.title('Prediction Distribution(KDE)   GRU',fontsize=7)\n",
    "# ax.tick_params(axis='both', which='both', length=0)\n",
    "# ax.set_yticks([])\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Nor_package')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f73fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab99dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Reshape X_train to have a time dimension\n",
    "\n",
    "X_train = np.array(aa_values_list)\n",
    "y_train = np.array(nor_package_values_list)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Build the improved RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(128, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(64, activation='relu', return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(32, activation='relu') ,\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predicted_activity = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predicted_activity)\n",
    "r2 = r2_score(y_test, predicted_activity)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
